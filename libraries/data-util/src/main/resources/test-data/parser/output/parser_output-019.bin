
tika-input-doc-021"PMultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation*ğÑMultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation Zijian Song School of CS, Peking University Beijing, China 2201111590@stu.pku.edu.cn Wenhan Zhang School of CS, Peking University Beijing, China pku_zwh@pku.edu.cn Lifang Deng Lazada Group Beijing, China wanmei.dlf@alibaba-inc.com Jiandong Zhang Lazada Group Beijing, China chensong.zjd@alibaba-inc.com Kaigui Bian School of CS, Peking University Beijing, China bkg@pku.edu.cn Bin Cui School of CS, Peking University Beijing, China bin.cui@pku.edu.cn Abstract To address the business needs of industrial recommendation sys- tems, an increasing number of Multi-Domain Recommendation (MDR) methods are designed to improve recommendation perfor- mance on multiple domains simultaneously. Most MDR methods follow a multi-task learning paradigm, suffering from poor deploy- ability and negative transfer. Due to the great success of large pre-trained models, the pre-train & fine-tune paradigm is attract- ing increasing attention. The latest methods introduce parameter- efficient fine-tuning techniques like prompt-tuning, showcasing high efficiency and effectiveness. However, these methods neglect the fundamental differences between recommendation and NLP tasks. The inadequate capacity of recommendation models restricts the effectiveness of prompts and adapters. Worse still, traditional natural domain division may group non-identically distributed sam- ples into the same domain, violating the assumption of independent and identically distributed (i.i.d.) data. In this paper, we propose MultiLoRA, aMulti-directional Low Rank Adaptation paradigm for multi-domain recommendation. First we pre-train a universal model using all data samples. Then we conduct multiple domain divisions on the sample space. Under each division, we fine-tune the pre-trained model to obtain a set of domain-specific LoRAs. Fi- nally, we learn a LoRA fusion module to integrate domain-specific preference patterns across multiple divisions. Experimental results on real-world datasets demonstrate notable advantages of Multi- LoRA: (1) achieving SOTA performance, (2) showcasing remarkable compatibility, and (3) proving highly efficient, featuring only 2% trainable parameters compared to the backbone. Zijian Song, Wenhan Zhang, Kaigui Bian, and Bin Cui are affiliated with School of CS, AI Innovation Center, National Engineering Laboratory for Big Data Analysis and Applications, Peking University. Lifang Deng, Jiandong Zhang are affiliated with Lazada Group. Kaigui Bian is also affiliated with State Key Laboratory of Multimedia Information Processing, Peking University. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0436-9/24/10 https://doi.org/10.1145/3627673.3679549 CCS Concepts â€¢ Information systems â†’ Recommender systems; â€¢ Comput- ing methodologies â†’ Transfer learning. Keywords Multi-Domain Recommendation, Click-Through Rate Prediction, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation ACM Reference Format: Zijian Song, Wenhan Zhang, Lifang Deng, Jiandong Zhang, Kaigui Bian, and Bin Cui. 2024. MultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation. In Proceedings of the 33rd ACM Interna- tional Conference on Information and Knowledge Management (CIKM â€™24), October 21â€“25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3627673.3679549 1 Introduction base model ğ›© stylized model ğ›©1 stylized model ğ›©2 multi-style model ğ›©{1,2} LoRA Î”ğ›©1 (animal) LoRA Î”ğ›©2 (impressionism) Text-to-Image Prompt: village, portrait, cat girl (a) Multiple LoRAs for Image Generation (b) Multiple LoRAs for Recommendation (Ours) universal model ğ›· model for task 1 model for task 2 model for composite task LoRA Î”ğ›·2 (clothing items) LoRA Î”ğ›·1 (female users) Figure 1: Comparison of using multiple LoRAs for image generation and recommendation. For recommendation, We design LoRA fusion module to calculate the combination weights instead of using manually assigned â€œ1â€. Click-Through Rate (CTR) prediction is an important problem in recommendation with widespread applications [2, 13, 25, 30, 31]. To meet the needs of modern recommendation platforms with mul- tiple domains, Multi-Domain Recommendation (MDR) has gained popularity in recent years to capture domain commonalities and 2148 https://orcid.org/0009-0005-7725-2497 https://orcid.org/0009-0007-3015-8596 https://orcid.org/0009-0003-6940-8303 https://orcid.org/0000-0003-3404-1559 https://orcid.org/0000-0003-0136-6082 https://orcid.org/0000-0003-1681-4677 https://doi.org/10.1145/3627673.3679549 https://doi.org/10.1145/3627673.3679549 http://crossmark.crossref.org/dialog/?doi=10.1145%2F3627673.3679549&domain=pdf&date_stamp=2024-10-21 CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Zijian Song et al. diversities concurrently. However, most existing MDR methods are based on the multi-task learning (MTL) paradigm, where a uni- fied model is trained to serve all domains [35]. These methods are usually tailored for specific business scenarios, resulting in poor compatibility and limited deployability [29]. For example, the star topology based on fully connected networks in STAR [27] may hinder its integration with additional modules like factorization machines. Additionally, MTL-based models receive supervision sig- nals from numerous domains, which inevitably leads to conflicts and negative transfer [37]. Worse still, most of them heavily rely on overlapping users or items [35], which is often unavailable in real-world applications. On the other hand, the pre-train & fine-tune paradigm is at- tracting increasing attention, inspired by the great success of large pre-trained models in natural language processing (NLP) and com- puter vision (CV) areas. Specifically, a universal model is pre-trained using data from all domains, then fine-tuned on the target domain. This paradigm is compatible with almost any modules or structures, and the model receives signals only from the target domain while fine-tuning, mitigating conflicts and overlapping reliance. How- ever, full fine-tuning constructs an entire domain-specific model for each domain, leading to high computation and storage costs. Fine-tuning on a small task may also cause catastrophic forget- ting, making the model forget common knowledge learned during pre-training [5, 21]. In contrast, Parameter-Efficient Fine-Tuning (PEFT) methods are more efficient and effective, fine-tuning only a small proportion of parameters. The latest MDR methods introduce prompt-tuning [14, 17] or adapters [10] to adapt the pre-trained model to the target domain. Specifically, PLATE [29] adds prompts before the input to help themodel better understand and accomplish domain-specific recommendation, while HAMUR [16] generates a trainable layer (i.e., an adapter) to process hidden states. They sub- tly prevent catastrophic forgetting and achieve better performance than full fine-tuning. However, these methods overlook the fundamental differences between recommendation and NLP tasks. Recommendation mod- els heavily rely on ID-based embeddings, which seriously hinders the development of general-purpose large-scale recommendation models [33]. Prompt-tuning only modifies the input, and adapters add just a single layer of domain-specific parameters, making them inadequate for learning new tasks when the pre-trained modelâ€™s capabilities are insufficient. In contrast, the Low-Rank Adaptation (LoRA) module [11], which updates all parameters, offers greater expressiveness in such cases. This is why LoRA is more commonly used in image generation than prompt-tuning. For example, as shown in Figure 1a, the diffusion model cannot achieve satisfactory output (anthropomorphic animals) through prompt adjustments without the help of LoRA, which is similar in the MDR context. Furthermore, there is another neglect in previous works that can be addressed by LoRA. The partitioning of domains is customarily based on a natural feature [15], such as the item category [20], or the recommendation scenario [27]. We refer to this partitioning as a Natural Domain Division, which may render samples with dif- ferent distributions in the same domain, violating the independent and identically distributed (i.i.d.) data assumption. For example, in Figure 2, if we use User Gender as the criterion, samples ğ‘Ÿ1 and ğ‘Ÿ2 would be assigned to different domains. This intuitively makes Sample ğ’“ğŸ user clicked item in the Front Page gender: age: â€¦â€¦ female young â€¦â€¦ User Profile name: category: â€¦â€¦ dress clothes â€¦â€¦ Item Profile Sample ğ’“ğŸ user clicked item in the Front Page gender: age: â€¦â€¦ male old â€¦â€¦ User Profile name: category: â€¦â€¦ jeans clothes â€¦â€¦ Item Profile Sample Space Division 1 criterion: user gender F M Division 2 item category Clothes Food Eletronics ğ’“ğŸ ğ’“ğŸ Figure 2: Depending on the criterion of domain division, non- identically distributed samples may be assigned to the same domain. sense, as male and female users indeed exhibit distinct preferences. However, if we use Item Category, they would coexist in the same domain, which violates the i.i.d. data assumption and compromises the performance. To alleviate this issue, we can use two criteria simultaneously, dividing the sample space into 2 Ã— 3 = 6 domains. However, this leads to a dramatic increase in domain amount, and exacerbates the already existing data sparsity issue. Fortunately, prior studies show that integrating multiple LoRAs can help diffusion models generate images with multiple styles [24, 36], as shown in Figure 1a. This is a capability that prompts and adapters do not possess, enabling us to learn composite recommen- dation tasks. Specifically, we introduce several Auxiliary Domain Divisions based on manually chosen criteria (for simplicity, we introduce only one in this paper). We then learn LoRAs in different directions respectively, and combine them to indirectly achieve a finer-grained domain division, as illustrated in Figure 1b. To address the challenges mentioned above, this paper proposes MultiLoRA, aMulti-directional Low Rank Adaptation paradigm for multi-domain recommendation. MultiLoRA is a pre-train and fine-tune paradigm compatible with most existing CTR methods, making it easy to deploy. It has three stages. First, we pre-train a unified recommendation model using all data to capture universal preference patterns. Second, we learn a set of domain-specific Lo- RAs under each domain division. Finally, we train a LoRA fusion module to combine multi-directional LoRAs, obtaining preference patterns on a finer-grained domain. The contribution of this study is summarized as follows. â€¢ We propose MultiLoRA, a plug-and-play training paradigm com- patible with most CTR prediction models, transforming them into powerful multi-domain recommendation models. â€¢ To the best of our knowledge, we are the first to introduce LoRA into recommendation systems. We design a novel LoRA module tailored for domain adaptation in MDR. It is computationally efficient, requiring learning only 2% of the parameters compared to the pre-trained model. This module addresses catastrophic forgetting, leading to enhanced recommendation performance. â€¢ To the best of our knowledge, we are the first to study the data distribution inconsistency issue in MDR. We devise a LoRA fu- sion module to combine domain-specific LoRAs trained under different domain divisions, obtaining preference patterns from various perspectives. â€¢ Through extensive experiments conducted on three real-world datasets, we demonstrate that MultiLoRA outperforms state-of- the-art methods in both efficiency and effectiveness. 2149 MultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA 2 Preliminary 2.1 Low-Rank Adaptation Low-Rank Adaptation (LoRA) is a technique that originally ap- peared in the natural language processing field, used to adapt large language models to downstream tasks. Hu et al. [11] claim that the change of model weights during the fine-tuning has a low intrinsic rank. Thus, it is possible to use low-rank matrix decomposition to approximate this change. Specifically, for a pre-trained weight ma- trixğ‘¾0 âˆˆ Rğ‘‘Ã—ğ‘˜ , its change during fine-tuning can be represented in a low-rank form: ğ‘¾0 + Î”ğ‘¾ =ğ‘¾0 + ğ‘¼ğ‘«, (1) where ğ‘« âˆˆ Rğ‘ŸÃ—ğ‘˜ is the downsampling matrix, and ğ‘¼ âˆˆ Rğ‘‘Ã—ğ‘Ÿ is the upsampling matrix, and ğ‘Ÿ â‰ª min(ğ‘‘, ğ‘˜). Linear algebra teaches us that the rank of ğ‘¼ğ‘« is at most ğ‘Ÿ . While fine-tuning, we learn matrices ğ‘¼ and ğ‘« to approximate Î”ğ‘Š . Later, LoRA was widely adopted in the field of image generation. By training on only a few images, LoRA can control the output style of diffusion models or help them generate images beyond their original capabilities. For instance, as shown in Figure 1, the base model cannot generate an anthropomorphic animal on its own, regardless of the text-to-image prompt used. However, with the help of a LoRA module trained on a few images, the model gains the ability to draw something it has never seen before. Our reasons for choosing LoRA are twofold: (1) While employing only low-rank matrices, LoRA is involved in the updates of a large number of parameters. In contrast, adapters and prompt-tuning only affect a small proportion of parameters, and their effectiveness heavily relies on the capabilities of the pre-trained model; (2) In image generation, multiple LoRAs can be combined to generate images that carry multiple styles, providing rich customizability [24, 36]. Hence, we can conduct multiple domain divisions, and two domain-specific LoRAs under different domain divisions can be easily combined to achieve finer-grained domain division. 2.2 Multi-Domain CTR Prediction A traditional CTR prediction model is trained on a single domain, with various features such as user profiles, item attributes, context information, and historical interactions as input ğ’™ . These raw fea- tures are then mapped into low-dimensional embedding ğ’†, fed into the subsequent prediction model to calculate the predicted CTR ğ‘¦. The ground truth label ğ‘¦ is then used to calculate the value of the loss function. The multi-domain CTR model takes an additional input, the domain indicator ğ‘‘ . The objective is to find a function F that can predict CTR ğ‘¦ for every domain: minğœ½ 1 ğ· ğ·âˆ‘ï¸ ğ‘‘=1 1 |Dğ‘‘ | |Dğ‘‘ |âˆ‘ï¸ ğ‘–=1 Lğ¶ğ‘‡ğ‘… (F (ğ’™ğ‘‘ğ‘– , ğ‘‘ ;ğœ½ ), ğ‘¦ğ‘‘ğ‘– ) (2) where ğœ½ denotes the parameters of the model, ğ· is the number of domains, Dğ‘‘ is the sample space of the ğ‘‘-th domain. 2.3 Typical Architecture of CTR Backbones A typical CTR prediction backbone consists of three components: embedding layer, feature interaction layer, and output layer [29, 34]. â€¦ user feature item feature context feature combination feature Input Embedding Layer Feature Interaction Logistic Regression à·œğ‘¦ Figure 3: Typical Click-Through Rate prediction backbone architecture. We only fine-tune the feature interaction layer to capture domain-specific preference patterns. 2.3.1 Embedding Layer. The raw features in the dataset can be cat- egorized into two types: sparse features, which include categorical information represented by one/multi-hot vectors; and dense fea- tures, which contain numerical information represented by scalars. Assuming the dataset contains ğ‘› sparse features ğ’›ğ‘– andğ‘š dense features ğ‘ ğ‘— , the input is represented as: ğ’™ = {ğ’›1, Â· Â· Â· , ğ’›ğ‘› ;ğ‘1, Â· Â· Â· , ğ‘ğ‘š}. (3) Sparse features are often mapped into a low-dimensional embed- ding by a look-up table: ğ’†ğ‘– = ğ‘¬ğ‘–ğ’›ğ‘– , where ğ‘¬ğ‘– âˆˆ Rğ‘¢ğ‘–Ã—ğ‘˜ is the mapping matrix of the ğ‘–-th feature field, and ğ‘¢ğ‘– represents the number of categories in the ğ‘–-th sparse feature, and ğ‘˜ is the dimensionality of embeddings. As for multi-hot sparse features, we conduct mean- pooling on embeddings of all non-zero feature values. For dense features, we apply normalization to ensure the consistency of the data distribution. The output of the embedding layer is the concate- nation of all feature fieldsâ€™ embeddings: ğ‘’ = [ğ‘’1, Â· Â· Â· , ğ‘’ğ‘›+ğ‘š] . (4) 2.3.2 Feature Interaction. This part captures the impact of interac- tions between various feature fields on the click-through rate. Its structure often reflects the fundamental differences among various CTR methods. Depending on the choice of the backbone network, the feature interaction layer can capture interactions between fea- ture fields that are explicit or implicit, low-dimensional or high- dimensional. For instance, Wide&Deep [3] is a widely used model that simultaneously captures linear feature combination and im- plicit high-dimensional feature interactions; DeepFM [6] captures both explicit second-order feature interactions and implicit high- dimensional interactions; DCN [28] captures both explicit and im- plicit high-dimensional interactions. Our fine-tuning is conducted on this layer to capture domain preference patterns. It is notewor- thy that some backbones contain normalization layers, which are unsatisfactory when dealing with multi-domain data with differ- ent distributions. Hence, during the fine-tuning stage, we instead employ the domain normalization layer like STAR [27]. Finally, we denote the output of the feature interaction layer as ğ’‰(ğ¿) , where ğ¿ is the depth of the feature interaction network. 2.3.3 Output Layer. This layer takes ğ’‰(ğ¿) as input and perform logistic regression to predict CTR: ğ‘¦ = ğœ (ğ‘¾ğ‘œğ’‰ (ğ¿) + ğ‘ğ‘œ ), (5) whereğ‘¾ğ‘œ , ğ‘ğ‘œ are trainable parameters, ğœ is sigmoid activation. 2150 CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Zijian Song et al. scaling & offset LoRA Backbone Embedding Layer Backbone Embedding Layer ğ· âˆ¼ ğ’©(0, ğœ2) ğ‘Ÿ ğ‘ˆ = 0 Backbone LoRA LoRA Fusion Embedding Layer Stage 1: Pretrain updated frozen Stage 2: Multi-Directional Low-Rank Adaptation Stage 3: LoRA Fusion âˆˆ ğ““ ğ’ğ’‚ğ’•ğ’–ğ’“ğ’‚ğ’ (or ğ““ ğ’‚ğ’–ğ’™ğ’Šğ’ğ’Šğ’‚ğ’“ğ’š ) âˆˆ ğ““ ğ’ğ’‚ğ’•ğ’–ğ’“ğ’‚ğ’ âˆ© ğ““ ğ’‚ğ’–ğ’™ğ’Šğ’ğ’Šğ’‚ğ’“ğ’š Figure 4: Overall architecture of MultiLoRA paradigm. In the pre-train stage, we train the backbone with data from all domains. In the multi-directional low-rank adaptation stage, we fine-tune the model to obtain domain-specific LoRAs under both natural and auxiliary domain divisions. In the fusion stage, we train a LoRA fusion module to integrate multi-directional parameter updates, and obtain the model for the composite task. At last, we employ Binary Cross Entropy (BCE) as the loss function to train the backbone, with the optimization objective expressed as follows: minğœ½L = âˆ’ 1 |T | | T |âˆ‘ï¸ ğ‘–=1 ğ‘¦ğ‘– log(ğ‘¦ğ‘– ) + (1 âˆ’ ğ‘¦ğ‘– ) log(1 âˆ’ ğ‘¦ğ‘– ), (6) where ğ‘¦ğ‘– is the ground truth label, and ğ‘¦ğ‘– is the predicted CTR of the ğ‘–-th sample, respectively, and T is the training sample space. 3 Method 3.1 Overview To address the limitation of existing multi-domain recommendation methods, we propose MultiLoRA paradigm, as shown in Figure 4. MultiLoRA is a plug-and-play training paradigm that can be applied tomost backbone networks that follow the framework in Section 2.3. It consists of following three stages. â€¢ Pretrain: We pre-train the backbone network with all samples in the dataset, capturing common preference patterns shared across domains. â€¢ Multi-Directional Low-Rank Adaptation: We freeze the em- bedding layer and the output layer, and fine-tune the feature interaction layer. Specifically, we employ parameter-efficient fine-tuning (PEFT) technique to low-rank adapt the pre-trained model to the target domain. Since naturally divided domains may contain non-identically distributed samples, we introduce an additional auxiliary domain division and train another set of LoRAs. The selection of the auxiliary domain division is further discussed in Section 4.5. â€¢ LoRA Fusion: Inspired by the stackability of LoRA observed in the image generation field, we aim to simultaneously utilize two sets of LoRAs for joint prediction. For this purpose, we train a LoRA fusion module. After the three stages of training, we can use both sets of LoRAs and the LoRA fusion module for joint prediction, or we can use only one set of LoRAs for prediction.We refer to the latter as SingleLoRA. 3.2 Multi-Directional Low-Rank Adaptation To retain the common knowledge learned during pre-training as much as possible, we freeze the embedding layer and the output layers during fine-tuning. We fine-tune only the feature interaction layer because it learns to identify crucial feature interactions, which reflect the preference patterns in that domain. Previous studies indicate that during continuous learning via fine-tuning, a model may forget knowledge acquired in previous tasks, a phenomenon known as catastrophic forgetting [5, 21]. For MDR, this means the model could forget previously acquired domain-shared patterns, thereby impairing its performance in the target domain. To avoid catastrophic forgetting, we introduce Low-Rank Adap- tation (LoRA). Hu et al. [11] claim that the change in weights during fine-tuning has a low intrinsic rank, making it possible to use low- rank matrix decomposition to approximate this change. Building upon their work, we enhanced the form of the low-rank matrix de- composition by adding scaling and offset terms, further boosting the expressiveness of the LoRA module. Specifically, for a pre-trained weight matrixğ‘¾0 âˆˆ Rğ‘‘Ã—ğ‘˜ , we assume that its update during fine- tuning can be represented in a simple low-rank form: ğ‘¾0 + Î”ğ‘¾ =ğ‘¾0 + (ğ’” âŠ— ğ‘¼ğ‘«) , (7) where ğ‘¼ âˆˆ Rğ‘‘Ã—ğ‘Ÿ , ğ‘« âˆˆ Rğ‘ŸÃ—ğ‘˜ , ğ’” âˆˆ Rğ‘‘ , and ğ‘Ÿ â‰ª min(ğ‘‘, ğ‘˜). âŠ— repre- sents the Hadamard product between ğ‘¼ğ‘« and broadcasted coeffi- cient vector ğ’”, such that the ğ‘–-th row of ğ‘¼ğ‘« gets multiplied with ğ‘–-th element of ğ’”. Similarly, for a pre-trained bias ğ’ƒ0 âˆˆ Rğ‘‘ , we learn its update ğ’ƒ . For the forward passing ğ’‰ =ğ‘¾0ğ’™ + ğ’ƒ0 in the pre-trained model, our modified version is: ğ’‰â€² = (ğ‘¾0 + Î”ğ‘¾ ) ğ’™ + (ğ’ƒ0 + ğ’ƒ) = (ğ‘¾0ğ’™ + ğ’ƒ0) + (ğ’” âŠ— ğ‘¼ğ‘«) ğ’™ + ğ’ƒ . (8) 2151 MultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA During fine-tuning,ğ‘¾0, ğ’ƒ0 are frozen, while ğ‘¼ ,ğ‘«, ğ’”, ğ’ƒ are trainable parameters of the LoRA module. We use random Gaussian initial- ization for ğ‘«, ğ’”, and zero for ğ‘¼ , ğ’ƒ . In this way, the update value (ğ’” âŠ— ğ‘¼ğ‘«) ğ’™ + ğ’ƒ is zero at the beginning of fine-tuning. It is noteworthy that, due to the distribution shifts across do- mains, traditional normalization is insufficient, and we maintain domain-specific batch normalization layers for each domain (like STAR [27]). Assuming the sample comes from domain ğ´, we have ğ’‰(ğ‘™ ) = DomNorm ( ğ’‰(ğ‘™ )0 + ğ’‰(ğ‘™ ) ğ´ ;ğ´, ğ‘™ ) , DomNorm (â„;ğ´, ğ‘™) = ğ›¾ (ğ‘™ ) ğ´ âŠ— â„ âˆ’ ğœ‡ğ´âˆšï¸ƒ ğœ2 ğ´ + ğœ– + ğ›½ (ğ‘™ ) ğ´ , (9) where ğ’‰(ğ‘™ )0 ,ğ’‰(ğ‘™ ) ğ´ are the ğ‘™-th hidden layer outputs of the pre-trained model and the LoRA, and ğ›¾ (ğ‘™ ) ğ´ , ğ›½ (ğ‘™ ) ğ´ are learnable parameters. Since natural domain division may assign non-identically dis- tributed samples to the same domain, and naive finer-grained do- main division exacerbates data sparsity, we manually choose a criterion and conduct an auxiliary domain division on the sample space. Under natural and auxiliary domain divisions, we train two sets of LoRAs pointing in different directions. 3.3 LoRA Fusion LoRA, along with diffusion models [9, 22], find extensive applica- tions in the image generation field 1. One of the most interesting applications of LoRA is that multiple LoRAs can be weightedly integrated to generate images that carry multiple styles, greatly improving customizability [24, 36]. Inspired by this, we aim to combine the domain-specific LoRAs obtained under multiple domain divisions to collectively contribute to more accurate recommendation. For image generation, the com- bination weights are usually manually determined [23]. We instead design a LoRA fusion module to calculate the weights. We suppose a sample ğ‘Ÿ belongs to domain ğ´ under the natural domain division and domain ğµ under the auxiliary domain division. Let the LoRA modules corresponding to domain ğ´ and ğµ be ğ¿ğ‘œğ‘…ğ´ğ´ and ğ¿ğ‘œğ‘…ğ´ğµ , containing domain-specific preference patterns from two different perspectives. We design a novel attention module to dynamically generate weights and blend these LoRA modules. As shown in Figure 5, we learn a LoRA fusion module at each layer of the backbone. The pre-trained modelâ€™s output serves as the query vector, and the two LoRAsâ€™ outputs serve as the key and value vectors. The output of the ğ‘™-th layerâ€™s LoRA fusion is: ğ’‰(ğ‘™ ) ğ¿ğ‘œğ‘…ğ´ = softmax ( ğ’’ (ğ‘™ ) ğ‘‡ ğ‘² (ğ‘™ ) ğ‘Ÿ ) ğ‘½ (ğ‘™ ) , ğ’’ (ğ‘™ ) =ğ‘¾ (ğ‘™ ) ğ‘„ ğ’‰(ğ‘™ )0 , ğ‘² (ğ‘™ ) =ğ‘¾ (ğ‘™ ) ğ¾ [ â„ (ğ‘™ ) ğ´ , â„ (ğ‘™ ) ğµ ] , ğ‘½ (ğ‘™ ) = [ â„ (ğ‘™ ) ğ´ , â„ (ğ‘™ ) ğµ ] , (10) where ğ‘¾ğ‘„ ,ğ‘¾ğ¾ âˆˆ Rğ‘ŸÃ—ğ‘˜ are trainable parameters; ğ’‰0,ğ’‰ğ´,ğ’‰ğµ are outputs of the pre-trained model, ğ¿ğ‘œğ‘…ğ´ğ´ , and ğ¿ğ‘œğ‘…ğ´ğµ , respectively. It is noteworthy that, to maintain the parameter-efficiency of our 1https://civitai.com/ Pretrained value softmax LoRA Fusion Add & Domain Norm LoRA LoRA query key Figure 5: Architecture of the LoRA fusion module. The out- puts of multiple LoRAs are aggregated through attention mechanism. For parameter-efficiency, we perform down- sampling on ğ’’ and ğ’Œ, while identity transformation on ğ’—. method,ğ‘¾ğ‘„ ,ğ‘¾ğ¾ are down-samplings that ğ‘Ÿ â‰ª ğ‘˜ , and we apply identity transformation on the value vector. At last, we sum the results of the pre-trained model and the LoRA fusion module, passing it through a domain batch normalization layer used only by samples âˆˆ ğ´ âˆ© ğµ: ğ’‰(ğ‘™ ) = DomNorm ( ğ’‰(ğ‘™ )0 + ğ’‰(ğ‘™ ) ğ¿ğ‘œğ‘…ğ´ ;ğ´ âˆ© ğµ, ğ‘™ ) . (11) By introducing the LoRA fusion module, we are able to integrate multi-directional domain-specific patterns, indirectly performing finer-grained domain division while avoiding proliferation of do- mains. This provides the prediction model with more information and alleviates the issue of non-identically distributed samples. 4 Experiments In this section, we conduct experiments on public real-world datasets to demonstrate the effectiveness of our proposed MultiLoRA and answer the following questions. â€¢ RQ1: How does MultiLoRA perform compared with other state- of-the-art baselines, especially PEFT-based methods? â€¢ RQ2: How do the modules of MultiLoRA affect its performance? â€¢ RQ3: Is MultiLoRA paradigm compatible with different recom- mendation backbones? Does the model performance decline when integrated with MultiLoRA? â€¢ RQ4: How to select a best auxiliary domain division? â€¢ RQ5: Does the MultiLoRA paradigm possess high computation and storage efficiency? 4.1 Experimental Settings 4.1.1 Datasets. Our experiments are conducted on three real-world datasets, namely MovieLens [7], Amazon [20], and Ali-CCP [19]. Amazon 5-core is already segmented based on item category. Ali- CCP is divided by recommendation scenarios. Since MovieLens has 2152 CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Zijian Song et al. Table 1: Statistics of datasets. Dataset MovieLens Amazon 5-core Ali-CCP Train Ali-CCP Test Domain #1 #2 #3 Clothing Beauty Health #1 #2 #3 #1 #2 #3 # Users 1325 2096 2619 39387 22363 38609 80704 1986 136406 47400 1156 73924 # Items 3429 3508 3595 23033 12101 18534 297046 109259 297733 291213 103080 295281 # Interaction 210747 395556 393906 278677 198502 346355 15885371 318873 26095661 16351580 321024 26344010 no explicit domain division, we follow the setting of HAMUR [16] to divide it into three domains based on user age. The statistics of sampled datasets are given in Table 1. â€¢ MovieLens2 is collected from the well-known online movie recommendation platform, MovieLens, encompassing 7 user fea- tures and 2 item features. We randomly divided it into training, validation, and test sets with a ratio of 8:1:1. â€¢ Amazon 5-core3 is a dense subset from the Amazon 2014 dataset, ensuring each user and item have a minimum of 5 associated records. We choose three related natural domains. The time range of validation set is between March 1, 2014, and April 30, 2014, while the records before 1st March 2014 and after 30th April 2014 are counted as training and test data. Only user ID, item ID, domain indicator, and ratings are used. Records with rating higher than 3 are considered as positive samples. â€¢ Ali-CCP4 is gathered from real-world traffic logs of the recom- mendation system in Taobao, comprising 13 user features, 5 item features, 4 combination features, 1 context feature, and 2 label features (click and purchase). We employ â€œclickâ€ as the target label. The context feature â€œ301â€, indicating the recommendation scenario, naturally divides the dataset into three domains. Ad- ditionally, we randomly split the original test dataset in half to create the validation and test sets. The principle of auxiliary domain division is to exhibit maximum domain divergence. In the case of MovieLens and Ali-CCP, we utilize user gender as the criterion for auxiliary domain division. For Amazon 5-core, we choose user activeness, namely whether they have posted more than ten reviews. Further discussion about auxiliary domain division is given in Section 4.5. 4.1.2 Baseline Methods. To showcase the superiority of our pro- posed MultiLoRA, we compare it with following baseline methods: â€¢ Single is trained only on the target domain. â€¢ Mix is trained on data from all domains, with all parameters shared across these domains. â€¢ Fine-tune is initially trained on data from all domains and sub- sequently fully fine-tuned on the target domain. â€¢ Shared Bottom [1] is a multi-task model with shared bottom layers. In our implementation, the embedding layer is shared. â€¢ MMoE [18] is a multi-task model built upon the Shared Bottom architecture. It features a set of bottom networks known as ex- perts and trains a gating network to dynamically select experts for each domain. 2https://grouplens.org/datasets/movielens/ 3http://jmcauley.ucsd.edu/data/amazon/ 4https://tianchi.aliyun.com/dataset/408 â€¢ STAR [27] is a multi-domain model employing a fully-connected network with a star topology. It consists of shared centralized parameters to capture general patterns and domain-specific pa- rameters to capture domain-specific patterns. â€¢ HAMUR [16] is a PEFT-based multi-domain sota method. It designs a pluggable module that can be seamlessly integrated into various existing multi-domain backbone models. It learns a domain-shared hyper-network to dynamically generate the adapter parameters for each domain. â€¢ PLATE [29] is another PEFT-based multi-domain SOTA method. It conducts prompt tuning with two prompt modules, namely domain prompt and user prompt, to capture domain distinctions and conduct more accurate personalized recommendation. 4.1.3 Evaluation Metrics. We employ two commonly used evalu- ation metrics, namely Area Under the ROC (AUC) and LogLoss, to assess CTR prediction model performance in our experiments. In general, a higher AUC value or a lower LogLoss at 0.001 level indicates significant better recommendation performance [6, 29]. 4.1.4 Implementation Details. All hyper-parameters were tuned on the validation sets. Learning rate is chosen from {1e-3, 5e-4, 1e-4}. Weight decay is chosen from {1e-4, 1e-5, 0}. Batch size is chosen from {512, 2048, 4096}. We use Adam optimizer with default settings, early stopping when AUC on validation set no longer increases for 5 epochs. The network structure hyper-parameters of all kinds of backbones are consistent, as outlined in Table 2. Table 2: The network structure hyper-parameters. Dataset MovieLens Amazon 5-core Ali-CCP Embedding Size 16 16 16 Hidden Layer [256,128] [1024,512,256,64] [1024,512,512,256,256,64,64] LoRA ğ‘Ÿ [8,8] [16,16,8,8] [32,32,32,16,16,8,8] LoRA fusion ğ‘Ÿ [8,8] [16,16,8,8] [32,32,32,16,16,8,8] In the overall performance comparison, for simplicity, we choose MLP as the backbone network for Single, Mix, Fine-tune, HAMUR, PLATE, SingleLoRA, and MultiLoRA. The compatibility experi- ment of MultiLoRA with more backbone networks are given in Section 4.4. 4.2 Performance Comparison (RQ1) To showcase the effectiveness of our proposed paradigm, we com- pare MultiLoRA, SingleLoRA and baseline methods on three real- world datasets, where SingleLoRA is the ablation version of Multi- LoRAwithout auxiliary domain division or LoRA fusion.We choose MLP as the backbone for simplicity. The overall performance is given in Table 3. We summarize the following observations: 2153 MultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Table 3: Overall performance of our proposed method and baselines on three real-world datasets. Boldface denotes the best result and underline indicates the best result among baselines. â˜… represents significance level ğ‘-value< 0.05 of comparing MultiLoRA over the best baselines. Model/AUC MovieLens Amazon 5-core Ali-CCP #1 #2 #3 Total Clothing Beauty Health Total #1 #2 #3 Total Single 0.7980 0.7963 0.7965 0.7969 0.6022 0.5682 0.5964 0.5901 0.5843 0.5363 0.5842 0.5694 Mix 0.8187 0.8117 0.8015 0.8105 0.6158 0.6197 0.6325 0.6243 0.6176 0.5908 0.6136 0.6153 Fine-tune 0.8147 0.8135 0.7979 0.8077 0.6195 0.5946 0.6244 0.6150 0.6208 0.5901 0.6194 0.6178 Shared Bottom 0.8077 0.8182 0.8025 0.8102 0.6059 0.5891 0.6078 0.6025 0.6202 0.5870 0.6174 0.6170 MMoE 0.8088 0.8120 0.8045 0.8097 0.6080 0.5867 0.6213 0.6089 0.6255 0.5822 0.6203 0.6173 STAR 0.8065 0.8064 0.8021 0.8055 0.6117 0.5853 0.6234 0.6108 0.6253 0.5887 0.6208 0.6198 HAMUR 0.8130 0.8159 0.8067 0.8121 0.6120 0.5861 0.6311 0.6148 0.6210 0.6062 0.6216 0.6210 PLATE 0.8134 0.8178 0.8063 0.8124 0.6182 0.6223 0.6426 0.6296 0.6229 0.6055 0.6234 0.6220 SingleLoRA 0.8212 0.8189 0.8061 0.8160 0.6134 0.6134 0.6290 0.6198 0.6225 0.6065 0.6224 0.6212 MultiLoRA 0.8223â˜… 0.8192â˜… 0.8097â˜… 0.8170â˜… 0.6297â˜… 0.6408â˜… 0.6542â˜… 0.6429â˜… 0.6324â˜… 0.6065 0.6245â˜… 0.6231â˜… Model/LogLoss MovieLens Amazon 5-core Ali-CCP #1 #2 #3 Total Clothing Beauty Health Total #1 #2 #3 Total Single 0.5387 0.5408 0.5326 0.6377 0.5442 0.5946 0.4581 0.5303 0.1711 0.2599 0.1630 0.1928 Mix 0.5188 0.5215 0.5215 0.5207 0.5202â˜… 0.4832 0.4383 0.4772 0.1662 0.1806 0.1601 0.1626 Fine-tune 0.5198 0.5179 0.5296 0.5229 0.5320 0.5132 0.4403 0.4912 0.1660 0.1837 0.1597 0.1629 Shared Bottom 0.5294 0.5139 0.5237 0.5210 0.5563 0.5310 0.4615 0.5114 0.1677 0.1833 0.1595 0.1628 MMoE 0.5244 0.5148 0.5209 0.5207 0.5498 0.5351 0.4583 0.5105 0.1671 0.1856 0.1592 0.1632 STAR 0.5257 0.5196 0.5218 0.5224 0.5523 0.5367 0.4567 0.5114 0.1673 0.1837 0.1589 0.1628 HAMUR 0.5281 0.5188 0.5199 0.5212 0.5507 0.5227 0.4481 0.5019 0.1653 0.1779 0.1653 0.1617 PLATE 0.5201 0.5155 0.5223 0.5198 0.5338 0.4833 0.4391 0.4806 0.1661 0.1786 0.1579 0.1615 SingleLoRA 0.5246 0.5167 0.5210 0.5201 0.5475 0.5105 0.4444 0.4948 0.1665 0.1781 0.1568â˜… 0.1601 MultiLoRA 0.5109â˜… 0.5114â˜… 0.5187â˜… 0.5147â˜… 0.5357 0.4817â˜… 0.4336â˜… 0.4771 0.1631â˜… 0.1754â˜… 0.1575 0.1587â˜… â€¢ Fine-tune shows significant performance decline in most cases compared to Mix, showing obvious catastrophic forgetting. This is particularly evident in the Ali-CCP dataset: Fine-tune signif- icantly outperforms Mix in domains #1 and #3, but its perfor- mance regresses on the much smaller domain #2, indicating a potential overfitting and forgetting of the common knowledge acquired during pre-training. â€¢ There are two baselines that employ PEFT techniques: PLATE based on prompt tuning and HAMUR based on adapter. Both methods exhibit inferior performance on the MovieLens dataset compared to the larger-scale Amazon and Ali-CCP datasets. This discrepancy arises because HAMURâ€™s dynamically generated adapter only engages with one layer of the model, limiting the knowledge expression of the hyper-network. Meanwhile, PLATE only fine-tunes the prefix of the input and the linear interaction layer, resulting in limited expressive capacity. Both methods heav- ily rely on the pre-trained modelâ€™s knowledge. In contrast, LoRA can be involved in the update of all parameters, making it less dependent on the original model. â€¢ Our proposedMultiLoRA outperforms all baselines across al- most all domains. This is because we not only use LoRA for fine-tuning, but also introduce auxiliary domain division. By combining the predictions of two sets of LoRAs, we indirectly yet effectively achieve finer-grained domain division, mitigating the adverse effects of non-identically distributed samples. 4.3 Ablation Study (RQ2) Table 3 have already included several ablation experiments. By comparingMix, Fine-tune and SingleLoRA, we demonstrate that full fine-tuning leads to catastrophic forgetting in some domains, and employing LoRA alleviates this issue. The fact thatMultiLoRA significantly outperforms SingleLoRA in most cases indicates that usingmultiple domain divisionsmitigates data distribution inconsis- tencies. Here, we conduct additional experiments. Let MultiLoRA- avg represent the variant of MultiLoRA where two LoRAs are averaged, andMultiLoRA-add represent the variant where two LoRAs are simply added. We compare their performance withMul- tiLoRA to illustrate the effectiveness of the LoRA fusion module. The results are shown in Table 4. 4.4 Compatibility with Backbone Models (RQ3) The MultiLoRA paradigm we propose is a training framework that, theoretically, can be combined with most CTR backbones to derive corresponding multi-domain models. We select three widely used methods, namely Wide&Deep [3], DCN [28], and DeepFM [6]. We integrate MultiLoRA with them and observe the impact on model performance. For each backbone, we compare three methods: Fine- tune, SingleLoRA, and MultiLoRA. The experimental results are shown in Table 5. We notice that, with our proposed MultiLoRA, the performance of all CTR back- bones is significantly improved compared to full fine-tuning. This 2154 CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Zijian Song et al. Table 4: Ablation study of the LoRA fusion module. Model/AUC MovieLens Amazon 5-core Ali-CCP #1 #2 #3 Total Clothing Beauty Health Total #1 #2 #3 Total MultiLoRA-add 0.7972 0.7992 0.7890 0.7958 0.5987 0.5678 0.5924 0.5870 0.5844 0.5467 0.5795 0.5713 MultiLoRA-avg 0.8156 0.8134 0.8019 0.8105 0.6113 0.6068 0.6271 0.6150 0.6235 0.6012 0.6178 0.6134 MultiLoRA 0.8223â˜… 0.8192â˜… 0.8097â˜… 0.8170â˜… 0.6297â˜… 0.6408â˜… 0.6542â˜… 0.6429â˜… 0.6324â˜… 0.6065â˜… 0.6245â˜… 0.6231â˜… Model/LogLoss MovieLens Amazon 5-core Ali-CCP #1 #2 #3 Total Clothing Beauty Health Total #1 #2 #3 Total MultiLoRA-add 0.5408 0.5477 0.5308 0.5697 0.5470 0.5947 0.4596 0.5339 0.1709 0.2606 0.1620 0.1938 MultiLoRA-avg 0.5265 0.5217 0.5230 0.5238 0.5466 0.5104 0.4470 0.5007 0.1688 0.1810 0.1593 0.1677 MultiLoRA 0.5109â˜… 0.5114â˜… 0.5206â˜… 0.5153â˜… 0.5357â˜… 0.4817â˜… 0.4336â˜… 0.4771â˜… 0.1631â˜… 0.1754â˜… 0.1575â˜… 0.1587â˜… Table 5: The results (AUC) of compatibility experiments on different recommendation backbones. Backbone Paradigm MovieLens Amazon 5-core Ali-CCP #1 #2 #3 Total Clothing Beauty Health Total #1 #2 #3 Total Wide&Deep Fine-tune 0.8197 0.8181 0.7972 0.8107 0.6199 0.5971 0.6268 0.6163 0.6238 0.5761 0.6246 0.6165 SingleLoRA 0.8245 0.8227 0.8057 0.8182 0.6149 0.6160 0.6268 0.6204 0.6248 0.6010 0.6212 0.6194 MultiLoRA 0.8267â˜… 0.8240â˜… 0.8092â˜… 0.8200â˜… 0.6305â˜… 0.6401â˜… 0.6497â˜… 0.6373â˜… 0.6356â˜… 0.6026â˜… 0.6265â˜… 0.6241â˜… DCN Fine-tune 0.8189 0.8147 0.7985 0.8095 0.6209 0.5981 0.6267 0.6173 0.6228 0.5947 0.6213 0.6203 SingleLoRA 0.8247 0.8188 0.8069 0.8169 0.6146 0.6147 0.6293 0.6211 0.6234 0.6075 0.6227 0.6222 MultiLoRA 0.8233â˜… 0.8201â˜… 0.8090â˜… 0.8182â˜… 0.6293â˜… 0.6402â˜… 0.6548â˜… 0.6399â˜… 0.6337â˜… 0.6085â˜… 0.6223 0.6236â˜… DeepFM Fine-tune 0.8258 0.8201 0.8008 0.8146 0.6223 0.5864 0.6255 0.6127 0.6223 0.5878 0.6134 0.6151 SingleLoRA 0.8307 0.8245â˜… 0.8067 0.8214 0.6176 0.6104 0.6280 0.6199 0.6242 0.6021 0.6238 0.6196 MultiLoRA 0.8333â˜… 0.8245â˜… 0.8110â˜… 0.8229â˜… 0.6277â˜… 0.6302â˜… 0.6382â˜… 0.6302â˜… 0.6378â˜… 0.6022 0.6316â˜… 0.6257â˜… Table 6: The performance change (in terms of AUC) of Multi- LoRA when selecting different auxiliary domain divisions. Overall, greater divergence in preference patterns among auxiliary domains, given the natural domain division, leads to better model performance. Criterion # domain Ali-CCP #1 #2 #3 Total User Gender 3 val 0.6336 0.6069 0.6243 0.6235 test 0.6324 0.6065 0.6245 0.6231 None (SingleLoRA) N/A val âˆ’0.0091 âˆ’0.0011 âˆ’0.0029 âˆ’0.0017 test âˆ’0.0099 0 âˆ’0.0021 âˆ’0.0019 User Occupation 3 val +0.0007 âˆ’0.0015 âˆ’0.0011 âˆ’0.0001 test +0.0013 âˆ’0.0009 âˆ’0.0005 âˆ’0.0004 User Age 4 val âˆ’0.0045 âˆ’0.0027 âˆ’0.0013 âˆ’0.0029 test âˆ’0.0036 âˆ’0.0023 +0.0008 âˆ’0.0017 User Geography 5 val âˆ’0.0054 âˆ’0.0045 âˆ’0.0033 âˆ’0.0041 test âˆ’0.0067 âˆ’0.0033 âˆ’0.0028 âˆ’0.0035 demonstrates MultiLoRAâ€™s outstanding ability to adapt to the target domain, preserving common knowledge acquired in pre-training and preventing catastrophic forgetting. Additionally, MultiLoRA exhibits compatibility with various backbones, making it easier to apply and deploy in various application scenarios. 4.5 Choice of Auxiliary Domain Division (RQ4) One crucial step for MultiLoRA is introducing auxiliary domain division. In this subsection, we examined how different auxiliary domain division criteria affect model performance on the Ali-CCP dataset. The Detailed results are presented in Table 6. We conduct evaluation on both the validation and test sets. It is noteworthy that, the results on these two sets are highly consistent. This consistency allows us to confidently select the best auxiliary domain division based on offline tests, which is very beneficial for industrial applications. We also notice that the same auxiliary domain division has dis- tinct effects for different natural domains. For example, when we use User Occupation as the auxiliary division criterion, the perfor- mance improves on domain #1, while decreasing for domain #2 and #3. This is possibly due to their different sensitivities to this auxiliary division criterion. As for the overall performance across the entire sample space, we have observed that an auxiliary domain division that exhibits larger domain divergence given the natural division is better. For instance, while users from different geographical regions show minor preference differences, significant differences exist between employed and unemployed users, given the natural domain division. Thus, the User Occupation criterion is better than User Geography in performance. This is because selecting such a criterion better alleviates the non-identically distributed data issue in naturally divided domains. 2155 MultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA 4.6 Efficiency Analysis (RQ5) MultiLoRA demonstrates outstanding parameter efficiency. For in- stance, when using MLP as the backbone, training a LoRA or the LoRA fusion module requires learning significantly fewer param- eters compared to full fine-tuning. The details can be found in Table 7, where the numbers in parentheses show the percentage of trainable parameters relative to the whole backbone and the feature interaction layer. Table 7: Statistics of trainable parameters. Module # Trainable Parameter MovieLens Amazon 5-core Ali-CCP Fully Fine-tune 275k 2.9M 26.4M Feature Interaction 63k 710k 1.4M LoRA 6k(2.19%,9.56%) 54k(1.84%,7.58%) 152k(0.57%,10.62%) LoRA Fusion 6k(2.15%,9.37%) 46k(1.57%,6.48%) 154k(0.58%,10.78%) We observe that the majority of parameters reside in the embed- ding layer. However, even when compared to the feature interaction layer, our approach only learns and stores one-tenth of the param- eters. Furthermore, by introducing auxiliary domain division and learning two sets of LoRAs and the LoRA fusion modules, we in- directly obtain domain-specific models for ğ‘›ğ‘š domains, where ğ‘› andğ‘š are the numbers of domains under the two domain divisions. This not only surpasses methods like STAR and fully fine-tuning that require 100% more trainable parameters for each domain, but also outperforms typical PEFT methods. 5 Related Work 5.1 Multi-Domain Recommendation Multi-domain recommendation is a type of cross-domain recom- mendation problem where the number of domains is greater than two. The MDR problem discussed in this paper is also known as multi-target cross-domain recommendation, which aims to improve recommendation performance on multiple domains simultaneously. Li et al. [15] summarized three different types of domains: sys- tem domain, data domain, and temporal domain. Specifically, for different system domains, there are different types of items or gen- res. For the data domain, they referred to user preferences for items stored in various forms of data, such as explicit ratings and implicit binary feedback. For the temporal domain, interaction records are divided into multiple periods based on timestamps, with each pe- riod constituting a domain. This domain division theory has been widely cited in subsequent researches [35]. There are various existing MDR methods proposed in recent years. For instance, MMoE [18] utilizes a set of bottom networks called experts, alongside a gating network to dynamically select ex- perts for each domain. STAR [27] adopts a fully-connected network with a star topology, while SAR-Net [26] employs two attention layers to capture user cross-domain interests. However, these ap- proaches adopt a multi-task training paradigm, often featuring complex structures, hindering integration with traditional back- bones and impeding deployment in real-world industrial settings. 5.2 Parameter-Efficient Fine-Tuning The concept of Parameter-Efficient Fine-Tuning (PEFT) was ini- tially proposed in natural language processing area [10]. With the significant increase in the scale of pre-trained language models in recent years, full fine-tuning has become cost-prohibitive. PEFT methods attract growing attention as they only require fine-tuning a small portion of parameters. Existing PEFT methods can be pri- marily categorized into three paradigms: adapter, prompt-tuning, and low-rank adaptation (LoRA) [8]. Adapter-based methods insert small neural modules called adapters into the pre-trained network, and only these adapters are trained during fine-tuning [10, 32]. Prompt tuning [14] and prefix tuning [17], inspired by the success of prompting methods, adds additional ğ‘™ tunable prefix tokens to the input or hidden layers, and only these soft prompts are trained. In the case of LoRA-based methods, Hu et al. [11] argue that the learned parameters in fine-tuning reside in a low intrinsic dimen- sion. Therefore, they learn low-rank matrices to appropriately up- date parameters [4, 12]. Some works apply PEFT techniques to recommendation sys- tems, but most aim to leverage large pre-trained models to obtain higher-quality feature vectors, exhibiting fundamental differences from our approach. Recently, HAMUR [16] learns a domain-shared hyper-network to dynamically generate the adapter parameters for each domain. However, it has to learn a backbone network for each domain, resulting in a high computational and storage cost. PLATE [29] employs prompt-tuning with two prompt mod- ules, namely domain prompt and user prompt, to capture domain distinctions and offer more accurate personalized recommenda- tions. However, the prompt tuning technique heavily relies on the generalization capability of the pre-trained model. 6 Conclusions In this paper, we identify the shortcomings of existingmulti-domain recommendation methods: MTL-based methods suffer from poor deployability and negative transfer, while PEFT-based methods overlook the fundamental differences between recommendation and NLP tasks, restricting the effectiveness of prompts and adapters. Additionally, we argue that the traditional natural domain division is insufficient, as it violates the independent and identically dis- tributed (i.i.d.) data assumption, leading to suboptimal performance. In response, we propose MultiLoRA, a multi-directional low-rank adaptation paradigm for multi-domain recommendation. We intro- duce LoRA, which is better suited for MDR tasks, offering efficiency (training only 2% of the parameters) and effectiveness (avoiding cat- astrophic forgetting). By leveraging the composable nature of LoRA, we combine preference patterns learned under different domain divisions by combining weight updates in different directions, indi- rectly achieving finer-grained domain division. Through extensive experiments, we demonstrate the superior performance of Multi- LoRA, surpassing state-of-the-art baselines on various backbones while showcasing its versatility and compatibility. 7 Acknowledgement This work is partially sponsored by National Key R&D Program of China under Grant 2022YFB3104200, NSFC (62032003), and research grant No. SH-2024JK29. 2156 CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Zijian Song et al. References [1] Rich Caruana. 1993. Multitask learning: A knowledge-based source of inductive bias1. In Proceedings of the Tenth International Conference on Machine Learning. 41â€“48. [2] Xusong Chen, Dong Liu, Zheng-Jun Zha, Wengang Zhou, Zhiwei Xiong, and Yan Li. 2018. Temporal hierarchical attention at category-and item-level for micro- video click-through prediction. In Proceedings of the 26th ACM international conference on Multimedia. 1146â€“1153. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7â€“10. [4] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2022. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650 (2022). [5] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128â€“135. [6] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [7] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1â€“19. [8] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 (2021). [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 6840â€“6851. [10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. PMLR, 2790â€“2799. [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [12] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. 2021. Fedpara: Low-rank hadamard product for communication-efficient federated learning. arXiv preprint arXiv:2108.06098 (2021). [13] Mostafa Kamal, Tarek Aziz Bablu, et al. 2022. Machine Learning Models for Pre- dicting Click-through Rates on social media: Factors and Performance Analysis. International Journal of Applied Machine Learning and Computational Intelligence 12, 4 (2022), 1â€“14. [14] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021). [15] Bin Li. 2011. Cross-domain collaborative filtering: A brief survey. In 2011 IEEE 23rd International Conference on Tools with Artificial Intelligence. IEEE, 1085â€“1086. [16] Xiaopeng Li, Fan Yan, Xiangyu Zhao, Yichao Wang, Bo Chen, Huifeng Guo, and Ruiming Tang. 2023. HAMUR: Hyper Adapter for Multi-Domain Recommenda- tion. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 1268â€“1277. [17] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021). [18] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-of- experts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1930â€“1939. [19] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 1137â€“1140. [20] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 43â€“52. [21] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in con- nectionist networks: The sequential learning problem. In Psychology of learning and motivation. Vol. 24. Elsevier, 109â€“165. [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684â€“10695. [23] S. Ryu. 2024. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning. https://github.com/cloneofsimo/lora Accessed: 2024-05-19. [24] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. 2023. ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs. arXiv:2311.13600 [cs.CV] [25] Pengyang Shao, Le Wu, Lei Chen, Kun Zhang, and Meng Wang. 2022. FairCF: Fairness-aware collaborative filtering. Science China Information Sciences 65, 12 (2022), 222102. [26] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. Sar-net: a scenario-aware ranking network for personalized fair recommendation in hundreds of travel scenarios. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 4094â€“4103. [27] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 4104â€“4113. [28] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDDâ€™17. 1â€“7. [29] Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huanshuo Liu, Yichao Wang, Rui Zhang, and Ruiming Tang. 2023. PLATE: A Prompt- Enhanced Paradigm for Multi-Scenario Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1498â€“1507. [30] Shitao Xiao, Yingxia Shao, Yawen Li, Hongzhi Yin, Yanyan Shen, and Bin Cui. 2022. LECF: recommendation via learnable edge collaborative filtering. Science China Information Sciences 65, 1 (2022), 112101. [31] Shuo Xiao, Dongqing Zhu, Chaogang Tang, and Zhenzhen Huang. 2023. Com- bining graph contrastive embedding and multi-head cross-attention transfer for cross-domain recommendation. Data Science and Engineering 8, 3 (2023), 247â€“262. [32] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. 2023. Aim: Adapting image models for efficient video action recognition. arXiv preprint arXiv:2302.03024 (2023). [33] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id- vs. modality-based recommender models revisited. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2639â€“2649. [34] Zhiyang Yuan, Wenguang Zheng, Peilin Yang, Qingbo Hao, and Yingyuan Xiao. 2023. Evolving interest with feature co-action network for CTR prediction. Data Science and Engineering 8, 4 (2023), 344â€“356. [35] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. A survey on cross-domain recommendation: taxonomies, methods, and future directions. ACM Transactions on Information Systems 41, 2 (2022), 1â€“39. [36] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. 2024. Multi-LoRA Composition for Image Generation. arXiv:2402.16843 [cs.CV] [37] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv preprint arXiv:2103.01696 (2021). 2157 https://github.com/cloneofsimo/lora https://arxiv.org/abs/2311.13600 https://arxiv.org/abs/2402.16843 Abstract 1 Introduction 2 Preliminary 2.1 Low-Rank Adaptation 2.2 Multi-Domain CTR Prediction 2.3 Typical Architecture of CTR Backbones 3 Method 3.1 Overview 3.2 Multi-Directional Low-Rank Adaptation 3.3 LoRA Fusion 4 Experiments 4.1 Experimental Settings 4.2 Performance Comparison (RQ1) 4.3 Ablation Study (RQ2) 4.4 Compatibility with Backbone Models (RQ3) 4.5 Choice of Auxiliary Domain Division (RQ4) 4.6 Efficiency Analysis (RQ5) 5 Related Work 5.1 Multi-Domain Recommendation 5.2 Parameter-Efficient Fine-Tuning 6 Conclusions 7 Acknowledgement ReferencesbÓ
i
x_tika_parsed_by_full_setLJorg.apache.tika.parser.DefaultParser; org.apache.tika.parser.pdf.PDFParser
.
pdf_docinfo_modified2025-05-23T22:45:48Z
æ

dc_subject×ÔMulti-Domain Recommendation, Click-Through Rate Prediction, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation; -  Information systems  ->  Recommender systems.-  Computing methodologies  ->  Transfer learning.
&
pdf_docinfo_custom_pdfversion1.5
-
pdf_docinfo_created2024-08-05T08:53:19Z
Š
pdf_docinfo_keywordsrpMulti-Domain Recommendation, Click-Through Rate Prediction, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation
³
xmp_creatortoolŸœLaTeX with acmart 2024/05/27 v2.08 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX
.
$access_permission_modify_annotationstrue
 
pdf_annotationsubtypesLink


pdf_hasxmptrue
+
	dc_formatapplication/pdf; version=1.5

pdf_hascollectionfalse
{
pdf_docinfo_subjectdb-  Information systems  ->  Recommender systems.-  Computing methodologies  ->  Transfer learning.


pdfversion1.5

xmptpg_npages10
~
"pdf_docinfo_custom_ptex_fullbannerXVThis is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0
^
dc_titleRPMultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation
)
dcterms_created2024-08-05T08:53:19Z

pdf_ocrpagecount0
+
!access_permission_extract_contenttrue
,
resourcenameacm_3627673_v1.3679549.pdf
.
$access_permission_can_print_faithfultrue
k
ptex_fullbannerXVThis is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0
¼
pdf_docinfo_creator_toolŸœLaTeX with acmart 2024/05/27 v2.08 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX
(
access_permission_fill_in_formtrue
5
+access_permission_extract_for_accessibilitytrue
%
pdf_containsnonembeddedfonttrue

pdf_annotationtypesnull
&
access_permission_can_modifytrue
`
x_tika_parsed_byLJorg.apache.tika.parser.DefaultParser; org.apache.tika.parser.pdf.PDFParser


pdf_hasxfafalse

pdf_num3dannotations0
D
)pdf_overallpercentageunmappedunicodechars2.0689798111561686E-4
!
content_typeapplication/pdf
-
#access_permission_assemble_documenttrue
$
pdf_docinfo_creatorZijian Song
g
pdf_docinfo_titleRPMultiLoRA: Multi-Directional Low-Rank Adaptation for Multi-Domain Recommendation
…
pdf_produceruspdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; modified using iText 4.2.0 by 1T3XT


dc_creatorZijian Song
%
pdf_totalunmappedunicodechars10

dc_languageen

pdf_encryptedfalse
"
pdf_containsdamagedfontfalse
*
dcterms_modified2025-05-23T22:45:48Z

pdf_pdfversion1.5

pdf_docinfo_trappedFalse
B
pdf_unmappedunicodecharsperpage0; 0; 0; 0; 10; 0; 0; 0; 0; 0
%
access_permission_can_printtrue

pdf_docinfo_produceruspdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; modified using iText 4.2.0 by 1T3XT

pdf_hasmarkedcontentfalse
P
pdf_charsperpage<:4330; 5952; 4826; 3564; 3947; 4697; 4595; 3976; 5363; 7083